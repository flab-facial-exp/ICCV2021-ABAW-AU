{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "443db9fa-bdf1-4191-8e5e-1f8ae7065698",
   "metadata": {},
   "source": [
    "# About this code\n",
    "- This is a code for ICCV2021 ABAW2 AU detection Challenge.\n",
    "- Copyright 2021 FUJITSU LIMITED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197386cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms, datasets\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pyper\n",
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import sklearn\n",
    "import sklearn.svm\n",
    "from sklearn.externals import joblib\n",
    "import scipy\n",
    "from scipy.optimize import differential_evolution\n",
    "from scipy.stats import gaussian_kde\n",
    "import importlib\n",
    "\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d8a54b",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f21329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_dir(dn):\n",
    "    if os.path.exists(dn):\n",
    "        shutil.rmtree(dn)\n",
    "    os.makedirs(dn)\n",
    "\n",
    "    \n",
    "def init_and_copy_dir( src_path, dst_path ):\n",
    "    if os.path.exists(dst_path):\n",
    "        shutil.rmtree(dst_path)\n",
    "        \n",
    "    shutil.copytree(src_path, dst_path) \n",
    "    \n",
    "def ICC(label_lst,pred_lst):\n",
    "    \n",
    "    data = []\n",
    "    for i in range(len(label_lst)):\n",
    "        data.append([label_lst[i],pred_lst[i]])\n",
    "    \n",
    "    data = np.array(data)\n",
    "    \n",
    "    r_env = pyper.R(use_numpy=True)\n",
    "    r_env(\"library(irr)\")\n",
    "    r_env.assign(\"data\",data)\n",
    "    r_env(\"value <- icc(data,model='twoway')$value\")\n",
    "    value = r_env.get(\"value\")\n",
    "    \n",
    "    if np.isnan(value):\n",
    "        value = -1000\n",
    "    \n",
    "    return value\n",
    "\n",
    "def MAE(label_lst,pred_lst):\n",
    "    \n",
    "    if len(label_lst) == 0:\n",
    "        return np.nan    \n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(label_lst)):\n",
    "        total += abs(label_lst[i] - pred_lst[i])\n",
    "\n",
    "    return total/len(label_lst)\n",
    "\n",
    "def RMSE(label_lst,pred_lst):\n",
    "    \n",
    "    if len(label_lst) == 0:\n",
    "        return np.nan\n",
    "    \n",
    "    total = 0\n",
    "    \n",
    "    for i in range(len(label_lst)):\n",
    "        total += (label_lst[i] - pred_lst[i])**2\n",
    "\n",
    "    return (total/len(label_lst))**0.5\n",
    "\n",
    "def measure_map( label_lst,pred_lst ):\n",
    "    \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    \n",
    "    for i in range(len(label_lst)):\n",
    "\n",
    "        if label_lst[i] == 1:     \n",
    "            if pred_lst[i] == 1:\n",
    "                TP += 1\n",
    "            elif pred_lst[i] == 0:\n",
    "                FN += 1\n",
    "            else:\n",
    "                raise\n",
    "        elif label_lst[i] == 0:         \n",
    "            if pred_lst[i] == 1:\n",
    "                FP += 1\n",
    "            elif pred_lst[i] == 0:\n",
    "                TN += 1\n",
    "            else:\n",
    "                raise\n",
    "        else:\n",
    "            raise                \n",
    "                \n",
    "                \n",
    "    return TP,FP,FN,TN\n",
    "\n",
    "\n",
    "def PRECISION( TP,FP,FN,TN ):    \n",
    "    \n",
    "    if TP + FP == 0:\n",
    "        return 0.0   \n",
    "\n",
    "    return TP / ( TP + FP )\n",
    "\n",
    "\n",
    "def RECALL( TP,FP,FN,TN ):\n",
    "\n",
    "    if TP + FN == 0:\n",
    "        return 0.0  \n",
    "    \n",
    "    return TP / ( TP + FN )\n",
    "\n",
    "def ACCURACY( TP,FP,FN,TN ):\n",
    "\n",
    "    if TP + FP + TN + FN == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return (TP+TN) / ( TP + FP + TN + FN )\n",
    "\n",
    "def F1( TP,FP,FN,TN ):\n",
    "    \n",
    "    recall = RECALL( TP,FP,FN,TN )\n",
    "    precision = PRECISION( TP,FP,FN,TN )\n",
    "    \n",
    "    if recall + precision == 0:\n",
    "        return 0.0\n",
    "\n",
    "    return 2 * recall * precision / ( recall + precision )\n",
    "\n",
    "\n",
    "def get_label_name( intensity, int_occ_mode ):\n",
    "    \n",
    "    if int_occ_mode == \"int\":\n",
    "        return \"INT%d\" % intensity\n",
    "    elif int_occ_mode == \"occ\":\n",
    "        if intensity == 0:\n",
    "            return \"NOOCC\"\n",
    "        elif intensity == 1:\n",
    "            return \"OCC\"\n",
    "        else:\n",
    "            raise\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "def get_measurement_score( label_lst, pred_lst, int_occ_mode ):\n",
    "    \n",
    "    if int_occ_mode == \"int\":\n",
    "        return ICC( label_lst, pred_lst )\n",
    "    else:\n",
    "        TP,FP,FN,TN = measure_map( label_lst,pred_lst )\n",
    "        return F1( TP,FP,FN,TN )        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022d24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img( img_name_0 ):\n",
    "    with open(img_name_0, 'rb') as f:\n",
    "        img_0 = Image.open(f)\n",
    "        img_0 = img_0.convert('RGB')\n",
    "\n",
    "    img_0 = transforms.ToTensor()(img_0)\n",
    "    img_0 = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(img_0)\n",
    "    \n",
    "    return img_0\n",
    "\n",
    "def get_best_epoch( filename, category, direct, param ):\n",
    "\n",
    "    fp = open(filename,\"r\")\n",
    "    epoch_score_lst = json.load( fp )\n",
    "    fp.close()\n",
    "    \n",
    "    if \"Valid\" in param.phase_lst:\n",
    "\n",
    "        pose_lst = list(param.dataset_path_lst[\"Valid\"].keys())\n",
    "        best_epoch = 0\n",
    "        for epoch in range(len(epoch_score_lst[\"Valid\"][pose_lst[0]][category])):\n",
    "\n",
    "            score_cur_best = np.average([epoch_score_lst[\"Valid\"][pose][category][best_epoch] for pose in pose_lst])\n",
    "            score = np.average([epoch_score_lst[\"Valid\"][pose][category][epoch] for pose in pose_lst])\n",
    "\n",
    "            if direct == \"UPPER\":\n",
    "                if score_cur_best < score:\n",
    "                    best_epoch = epoch\n",
    "            elif direct == \"LOWER\":\n",
    "                if score_cur_best > score:\n",
    "                    best_epoch = epoch\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        pose_lst = list(param.dataset_path_lst[\"Train\"].keys())\n",
    "        best_epoch = len(epoch_score_lst[\"Train\"][pose_lst[0]][category])-1\n",
    "                \n",
    "                \n",
    "                \n",
    "    print(\"best_epoch\",best_epoch)\n",
    "      \n",
    "    print(\"---\")\n",
    "    \n",
    "    for phase in epoch_score_lst.keys():\n",
    "        pose_lst = list(param.dataset_path_lst[phase].keys())\n",
    "        for c in epoch_score_lst[phase][pose_lst[0]].keys():\n",
    "            \n",
    "            score_lst = [epoch_score_lst[phase][pose][c][best_epoch] for pose in pose_lst]\n",
    "            print(phase,c,\"AVERAGE:\",np.average(score_lst),\"STD:\",np.std(score_lst))\n",
    "            \n",
    "    return best_epoch\n",
    "    \n",
    "def show_score_lst( filename, dataset_path_lst ):\n",
    "\n",
    "    fp = open(filename,\"r\")\n",
    "    epoch_score_lst = json.load( fp )\n",
    "    fp.close()\n",
    "    \n",
    "    pose_lst = list(dataset_path_lst[\"Train\"].keys())\n",
    "    for c in epoch_score_lst[\"Train\"][pose_lst[0]].keys():\n",
    "        for phase in epoch_score_lst.keys():\n",
    "            pose_lst = list(dataset_path_lst[phase].keys())\n",
    "            en = len(epoch_score_lst[phase][pose_lst[0]][c])\n",
    "            score_lst = [ np.average([ epoch_score_lst[phase][pose][c][epoch] for pose in pose_lst ]) for epoch in range(en) ]\n",
    "            \n",
    "            plt.plot(list(range(len(score_lst))),score_lst,label=\"%s %s\" % (phase,c))\n",
    "\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0)\n",
    "        plt.title(c)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8367e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairwise_net_best_epoch( param ):\n",
    "\n",
    "    filename = \"./log/pair_net_epoch_score_lst_%s_%s.json\" % (param.au_name,\"posesall\")\n",
    "\n",
    "    pair_net_best_epoch = get_best_epoch( filename, \"LOSS\", \"LOWER\", param )\n",
    "    show_score_lst( filename, param.dataset_path_lst )\n",
    "\n",
    "    return pair_net_best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0be02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_param():\n",
    "    \n",
    "    import param    \n",
    "    importlib.reload(param)\n",
    "    \n",
    "    param.device = torch.device(param.map_location)\n",
    "\n",
    "    return param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611d0dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enable_img_filename_lst( input_filename_lst, enable_subject_lst ):\n",
    "    \n",
    "    output_filename_lst = []\n",
    "    \n",
    "    for input_filename in input_filename_lst:\n",
    "        subject = os.path.basename(input_filename).split(\"_\")[2]\n",
    "        \n",
    "        if subject in enable_subject_lst:\n",
    "            output_filename_lst.append(input_filename)\n",
    "            \n",
    "    return output_filename_lst\n",
    "\n",
    "def get_img_filename_lst( dataset_name, dataset_path, au_name, intensity, phase, param ):\n",
    "    \n",
    "    dataset_path_dir = os.path.basename(os.path.dirname(dataset_path))\n",
    "\n",
    "    if dataset_path_dir.find( \"frames_for_pytorch_procrustes_occ\" ) == 0:\n",
    "\n",
    "        occ_int_flag = \"occ\"\n",
    "\n",
    "        if intensity > 1:\n",
    "            filename_lst = []\n",
    "        else:\n",
    "            dirname = dataset_path + \"/\" + au_name + \"/\" + get_label_name( intensity, \"occ\" )\n",
    "            filename_lst = glob.glob(dirname + \"/*.jpg\")\n",
    "\n",
    "    elif dataset_path_dir.find( \"frames_for_pytorch_procrustes_int\" ) == 0:\n",
    "\n",
    "        occ_int_flag = \"int\"\n",
    "        \n",
    "        dirname = dataset_path + \"/\" + au_name + \"/\" + get_label_name( intensity, \"int\" )\n",
    "        filename_lst = glob.glob(dirname + \"/*.jpg\")\n",
    "\n",
    "    elif dataset_path_dir.find( \"labels_for_pytorch_procrustes_occ\" ) == 0:\n",
    "\n",
    "        occ_int_flag = \"occ\"\n",
    "\n",
    "        if intensity > 1:\n",
    "            filename_lst = []\n",
    "        else:\n",
    "            filename_lst = []\n",
    "            dirname = dataset_path + \"/\" + au_name\n",
    "            for csv_filename in glob.glob(dirname+\"/*.csv\"):\n",
    "\n",
    "                csv_basename = \"_\".join(os.path.basename(csv_filename).split(\"_\")[:5])\n",
    "\n",
    "                for line in open(csv_filename):\n",
    "\n",
    "                    frame_id, intensity_tmp = line.rstrip().split(\",\")\n",
    "                    intensity_tmp = int(intensity_tmp)\n",
    "\n",
    "                    if param.enable_pair_dataset_equal_test and phase == \"Test\":\n",
    "                        if not 0 <= intensity_tmp <= 5:\n",
    "                            intensity_tmp = 0\n",
    "\n",
    "\n",
    "                    if intensity_tmp != intensity:\n",
    "                        continue\n",
    "\n",
    "                    img_filename = param.imgs_top_dirname_lst[dataset_name] + \"/\" + csv_basename + \"_\" + frame_id + \".jpg\"\n",
    "\n",
    "                    if not os.path.exists(img_filename):\n",
    "                        continue                            \n",
    "\n",
    "                    filename_lst.append(img_filename)\n",
    "                    \n",
    "    elif dataset_path_dir.find( \"labels_for_pytorch_procrustes_int\" ) == 0:\n",
    "\n",
    "        occ_int_flag = \"int\"\n",
    "\n",
    "        filename_lst = []\n",
    "        dirname = dataset_path + \"/\" + au_name\n",
    "        for csv_filename in glob.glob(dirname+\"/*.csv\"):\n",
    "\n",
    "            csv_basename = \"_\".join(os.path.basename(csv_filename).split(\"_\")[:5])\n",
    "\n",
    "            for line in open(csv_filename):\n",
    "\n",
    "                frame_id, intensity_tmp = line.rstrip().split(\",\")\n",
    "                intensity_tmp = int(intensity_tmp)\n",
    "\n",
    "                if param.enable_pair_dataset_equal_test and phase == \"Test\":\n",
    "                    if not 0 <= intensity_tmp <= 5:\n",
    "                        intensity_tmp = 0\n",
    "\n",
    "                if intensity_tmp != intensity:\n",
    "                    continue\n",
    "\n",
    "                img_filename = param.imgs_top_dirname_lst[dataset_name] + \"/\" + csv_basename + \"_\" + frame_id + \".jpg\"\n",
    "                \n",
    "                if not os.path.exists(img_filename):\n",
    "                    continue                            \n",
    "\n",
    "                filename_lst.append(img_filename)\n",
    "\n",
    "    else:\n",
    "        raise \n",
    "        \n",
    "    return filename_lst, occ_int_flag\n",
    "\n",
    "def get_filename_lst_cache(dataset_path_lst,enable_subject_lst,phase,param):\n",
    "\n",
    "    au_name = param.au_name\n",
    "    pair_net_divide_task = param.pair_net_divide_task\n",
    "    \n",
    "    filename_lst_cache_all = []\n",
    "    for dataset_name,dataset_path_ll in dataset_path_lst.items():\n",
    "        filename_lst_cache = {}\n",
    "        occ_int_flag = \"int\"\n",
    "        pose_mode = \"single_pose\"\n",
    "        pose_prev = None\n",
    "        \n",
    "        for intensity in range(6):\n",
    "            filename_lst_cache[intensity] = {}\n",
    "            \n",
    "            for dataset_path in dataset_path_ll:    \n",
    "\n",
    "\n",
    "                filename_lst, occ_int_flag = get_img_filename_lst( dataset_name, dataset_path, au_name, intensity, phase, param )                \n",
    "\n",
    "                for filename in sorted(extract_enable_img_filename_lst(filename_lst,enable_subject_lst[dataset_name])):\n",
    "                    \n",
    "                    if pair_net_divide_task:\n",
    "                        subject = os.path.basename(filename).split(\"_\")[2] + \"_\" + os.path.basename(filename).split(\"_\")[3]\n",
    "                    else:\n",
    "                        subject = os.path.basename(filename).split(\"_\")[2]\n",
    "                    \n",
    "                    pose = int(os.path.basename(filename).split(\"_\")[4])\n",
    "                    frame_id = int(os.path.splitext(os.path.basename(filename))[0].split(\"_\")[5][len(\"frame\"):])\n",
    "\n",
    "                    if (not pose_prev is None) and (pose != pose_prev):\n",
    "                        pose_mode = \"multi_pose\"\n",
    "                    pose_prev = pose\n",
    "\n",
    "                    if not subject in filename_lst_cache[intensity]:\n",
    "                        filename_lst_cache[intensity][subject] = [[],[],[]]\n",
    "\n",
    "                    filename_lst_cache[intensity][subject][0].append(filename)\n",
    "                    filename_lst_cache[intensity][subject][1].append(pose)\n",
    "                    filename_lst_cache[intensity][subject][2].append(frame_id)\n",
    "                \n",
    "        filename_lst_cache_all.append((filename_lst_cache,occ_int_flag,pose_mode,dataset_name))\n",
    "        \n",
    "    return filename_lst_cache_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7426f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_for_pair_net( param ):\n",
    " \n",
    "    for phase in param.phase_lst:\n",
    "        for pose in param.dataset_path_lst[phase].keys():\n",
    "\n",
    "            csv_filename = param.dataset_lst_dirname + \"/AU_INT_PAIR_%s_%s_%s.csv\" % (param.au_name,phase,pose)\n",
    "\n",
    "            print(\"---------------------\")\n",
    "\n",
    "            print(csv_filename)\n",
    "\n",
    "            filename_lst_cache_all = get_filename_lst_cache(param.dataset_path_lst[phase][pose],param.enable_subject_lst[phase][pose],phase,param)\n",
    "\n",
    "            for i in range(len(filename_lst_cache_all)):\n",
    "                print(\"filename_lst_cache_all\",filename_lst_cache_all[i][1],filename_lst_cache_all[i][2],filename_lst_cache_all[i][3])\n",
    "\n",
    "\n",
    "            fp = open(csv_filename,\"w\")\n",
    "\n",
    "            counter = 0\n",
    "            while counter < param.dataset_num_lst[phase]:\n",
    "\n",
    "                dataset_id = random.sample(list(range(len(filename_lst_cache_all))),1)[0]\n",
    "                filename_lst_cache,occ_int_flag,pose_mode,_ = filename_lst_cache_all[dataset_id]\n",
    "\n",
    "                intensity_pair_lst = []\n",
    "                for intensity_0 in range(2 if occ_int_flag == \"occ\" else 6):\n",
    "                    for intensity_1 in range(2 if occ_int_flag == \"occ\" else 6):\n",
    "\n",
    "                        if param.enable_pair_dataset_equal_test and phase == \"Test\":\n",
    "                            pass\n",
    "\n",
    "                        else:                        \n",
    "                            if abs(intensity_0 - intensity_1) == 0:\n",
    "                                continue\n",
    "\n",
    "\n",
    "                        intensity_pair_lst.append((intensity_0,intensity_1))\n",
    "\n",
    "                target_intensity_pair = random.sample(intensity_pair_lst,1)[0]               \n",
    "\n",
    "\n",
    "                intensity_0 = target_intensity_pair[0]\n",
    "                intensity_1 = target_intensity_pair[1]\n",
    "\n",
    "\n",
    "                if intensity_0 < intensity_1:\n",
    "                    label = 1\n",
    "                elif intensity_0 > intensity_1:\n",
    "                    label = 0\n",
    "                else:\n",
    "                    if param.enable_pair_dataset_equal_test and phase == \"Test\":\n",
    "                        label = 0\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "\n",
    "                subject_lst_0 = sorted(list(filename_lst_cache[intensity_0].keys()))\n",
    "                subject_lst_1 = sorted(list(filename_lst_cache[intensity_1].keys()))\n",
    "\n",
    "\n",
    "                if len(subject_lst_0) == 0:\n",
    "                    continue\n",
    "                if len(subject_lst_1) == 0:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                pair_lst = []\n",
    "                \n",
    "                if param.pair_net_dataset_only_same_subject:\n",
    "\n",
    "                    for s0 in subject_lst_0:\n",
    "                        for s1 in subject_lst_1:\n",
    "                            if s0 == s1:\n",
    "                                pair_lst.append((s0,s1))\n",
    "                else:\n",
    "                    \n",
    "                    for s0 in subject_lst_0:\n",
    "                        for s1 in subject_lst_1:\n",
    "                            pair_lst.append((s0,s1))\n",
    "\n",
    "\n",
    "\n",
    "                if len(pair_lst) == 0:\n",
    "                    continue\n",
    "\n",
    "\n",
    "                subject_0,subject_1 = random.sample(pair_lst,1)[0]\n",
    "\n",
    "\n",
    "                filename_lst_0,pose_lst_0,frame_id_lst_0 = filename_lst_cache[intensity_0][subject_0]\n",
    "                filename_lst_1,pose_lst_1,frame_id_lst_1 = filename_lst_cache[intensity_1][subject_1]\n",
    "\n",
    "\n",
    "                if len(filename_lst_0) == 0:\n",
    "                    continue\n",
    "                if len(filename_lst_1) == 0:\n",
    "                    continue\n",
    "\n",
    "                filename_0 = random.sample(filename_lst_0,1)[0]\n",
    "                filename_1 = random.sample(filename_lst_1,1)[0]\n",
    "\n",
    "                fp.write(\"%s,%s,%d\\n\" % (os.path.abspath(filename_0),os.path.abspath(filename_1),label))\n",
    "                counter += 1\n",
    "\n",
    "\n",
    "            fp.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7025fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_for_pair_net_prediction( param ):\n",
    "  \n",
    "    for phase in param.phase_lst:\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "            csv_filename = param.dataset_lst_dirname + \"/AU_INT_TUNE_%s_%s_%s.csv\" % (param.au_name,phase,pose)\n",
    "\n",
    "            print(csv_filename)\n",
    "\n",
    "            fp = open(csv_filename,\"w\")\n",
    "\n",
    "            for dataset_name in param.intensity_tune_dataset_path_lst[phase][pose].keys():\n",
    "\n",
    "                for cdirname in param.intensity_tune_dataset_path_lst[phase][pose][dataset_name]:\n",
    "                    filename_lst = {}\n",
    "\n",
    "                    for intensity in range(param.intensity_level):\n",
    "                        \n",
    "                        fl,_ = get_img_filename_lst( dataset_name, cdirname, param.au_name, intensity, phase, param )\n",
    "                        \n",
    "                        for filename in sorted(extract_enable_img_filename_lst(fl, param.enable_subject_lst[phase][pose][dataset_name])):\n",
    "\n",
    "                            subject_task_pose = os.path.basename(filename).split(\"_\")[2] + \"_\" + os.path.basename(filename).split(\"_\")[3] + \"_\" + os.path.basename(filename).split(\"_\")[4]\n",
    "                            if not subject_task_pose in filename_lst:\n",
    "                                filename_lst[subject_task_pose] = []\n",
    "                            filename_lst[subject_task_pose].append((filename,intensity))\n",
    "\n",
    "\n",
    "                    for subject_task_pose, filename_intensity_lst in filename_lst.items():\n",
    "\n",
    "                        if param.intensity_tune_sampling_max <= 0:\n",
    "                            sampling_num = len(filename_intensity_lst)\n",
    "                        else:\n",
    "                            sampling_num = min(param.intensity_tune_sampling_max,len(filename_intensity_lst))\n",
    "\n",
    "\n",
    "\n",
    "                        for filename,intensity in random.sample(filename_intensity_lst,sampling_num):\n",
    "                            fp.write(\"%s,%d\\n\" % (os.path.abspath(filename),intensity))\n",
    "            fp.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a10dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset( param ):\n",
    "    make_dataset_for_pair_net( param )\n",
    "    make_dataset_for_pair_net_prediction( param )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da54af26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PairDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file,header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name_0 = self.df.iloc[idx,0]\n",
    "        img_name_1 = self.df.iloc[idx,1]\n",
    "        label = self.df.iloc[idx,2]\n",
    "       \n",
    "        img_0 = load_img( img_name_0 )\n",
    "        img_1 = load_img( img_name_1 )\n",
    "\n",
    "        return img_0,img_1,label,img_name_0,img_name_1\n",
    "    \n",
    "def get_dataloaders_for_pairwise_net( param ):\n",
    "\n",
    "    dataloaders = {}\n",
    "\n",
    "    for phase in param.phase_lst:\n",
    "        dataloaders[phase] = {}\n",
    "        for pose in param.dataset_path_lst[phase].keys():\n",
    "\n",
    "            csv_filename = param.dataset_lst_dirname + \"/AU_INT_PAIR_%s_%s_%s.csv\" % (param.au_name,phase,pose)\n",
    "\n",
    "            pair_dataset = PairDataset(csv_filename)\n",
    "            dataloader = torch.utils.data.DataLoader(pair_dataset, batch_size=param.batch_size_bin, shuffle=True, num_workers=1)\n",
    "\n",
    "            dataloaders[phase][pose] = dataloader\n",
    "                \n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfb7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_sigmoid(x,epsilon):\n",
    "    return (1/(1+torch.exp(-x)))*(1-2*epsilon) + epsilon\n",
    "\n",
    "def torch_logit(x):\n",
    "    return torch.log(x/(1-x))\n",
    "\n",
    "\n",
    "class PairNet(nn.Module):\n",
    "    def __init__(self,param):\n",
    "        super(PairNet, self).__init__()\n",
    "        \n",
    "        self.pair_net_conf_mode = param.pair_net_conf_mode\n",
    "        \n",
    "        classifier_dim = 1\n",
    "        if self.pair_net_conf_mode == \"integrated\":\n",
    "            classifier_dim += 1\n",
    "        \n",
    "        \n",
    "        self.epsilon = 1.0e-3\n",
    "\n",
    "\n",
    "        self.model = copy.deepcopy(models.vgg16(pretrained=True))\n",
    "        for name, pm in self.model.named_parameters():\n",
    "            if ('features.0.' in name or 'features.2.' in name):\n",
    "                pm.requires_grad = False\n",
    "        self.model.classifier[6] = nn.Linear(4096, classifier_dim)\n",
    "\n",
    "        \n",
    "        if self.pair_net_conf_mode == \"separated\":        \n",
    "            self.model_conf = copy.deepcopy(models.vgg16(pretrained=True))\n",
    "            for name, pm in self.model_conf.named_parameters():\n",
    "                if ('features.0.' in name or 'features.2.' in name):\n",
    "                    pm.requires_grad = False\n",
    "            self.model_conf.classifier[6] = nn.Linear(4096, classifier_dim)\n",
    "        \n",
    "        \n",
    "    def freeze_model( self ):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            param.requires_grad = False            \n",
    "        \n",
    "        \n",
    "    def forward(self, img_0, img_1):\n",
    "        \n",
    "        x_0,conf_0 = self.forward_single(img_0)\n",
    "        x_1,conf_1 = self.forward_single(img_1)\n",
    "        \n",
    "        return x_0, x_1, conf_0, conf_1\n",
    "    \n",
    "\n",
    "    def forward_only_pseudo(self, img_0, img_1):\n",
    "        x_0 = self.model.forward(img_0)\n",
    "        x_1 = self.model.forward(img_1)\n",
    "        conf_0 = None\n",
    "        conf_1 = None\n",
    "        \n",
    "        return x_0, x_1, conf_0, conf_1\n",
    "\n",
    "    def forward_single(self, img_0):\n",
    "        \n",
    "        if self.pair_net_conf_mode == \"disabled\":\n",
    "            x_0 = self.model.forward(img_0)\n",
    "            conf_0 = None\n",
    "        \n",
    "        elif self.pair_net_conf_mode == \"integrated\":        \n",
    "            _x_0 = self.model.forward(img_0)\n",
    "           \n",
    "            x_0 = torch.unsqueeze(_x_0[:,0],1)\n",
    "            conf_0 = torch.unsqueeze(_x_0[:,1],1)\n",
    "\n",
    "            conf_0 = torch.clamp( conf_0, min=-10, max=10 )\n",
    "            conf_0 = torch.exp(conf_0)\n",
    "\n",
    "        elif self.pair_net_conf_mode == \"separated\":\n",
    "            x_0 = self.model.forward(img_0)\n",
    "            conf_0 = self.model_conf.forward(img_0)\n",
    "            \n",
    "            conf_0 = torch.clamp( conf_0, min=-10, max=10 )\n",
    "            conf_0 = torch.exp(conf_0)\n",
    "        else:\n",
    "            raise\n",
    "            \n",
    "        return x_0, conf_0\n",
    "            \n",
    "    \n",
    "def my_loss_func(x_0, x_1, conf_0, conf_1, label, conf_mode, param ):\n",
    "    \n",
    "    x_0 = torch.squeeze(x_0,1)\n",
    "    x_1 = torch.squeeze(x_1,1)\n",
    "\n",
    "\n",
    "    label = label*2-1\n",
    "    \n",
    "    \n",
    "    \n",
    "    if conf_mode:\n",
    "        \n",
    "        conf_0 = torch.squeeze(conf_0,1)\n",
    "        conf_1 = torch.squeeze(conf_1,1)\n",
    "        \n",
    "        m = param.pair_net_loss_m_conf\n",
    "        \n",
    "        p = 1 - 0.5*(1+torch.erf( ( m - (x_1 - x_0) * label) / torch.sqrt( 2*(conf_0*conf_0 + conf_1*conf_1)) ) )\n",
    "        p = torch.clamp(p,min=param.epsilon)    \n",
    "        loss_element = -torch.log(p)\n",
    "      \n",
    "        \n",
    "    else:\n",
    "        m = param.pair_net_loss_m\n",
    "        loss_element = torch.clamp(m - (x_1 - x_0)*label, min=0)   \n",
    "    \n",
    "    loss = torch.mean(loss_element)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7beb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pairwise_net( param, separate_conf_mode, pair_net_best_filename=None ):\n",
    "\n",
    "    print(\"=====================\")\n",
    "    print(\"au_name\",param.au_name)\n",
    "    \n",
    "    dataloaders = get_dataloaders_for_pairwise_net( param )\n",
    "\n",
    "    pair_net = PairNet( param )\n",
    "    if separate_conf_mode:\n",
    "        pair_net.load_state_dict(torch.load(pair_net_best_filename))\n",
    "        pair_net.freeze_model()    \n",
    "    pair_net = pair_net.to(param.device)\n",
    "\n",
    "    if param.pairwise_net_optimizer == \"Adam\":\n",
    "        optimizer = optim.Adam(pair_net.parameters(), lr=0.00005)\n",
    "    elif param.pairwise_net_optimizer == \"AdamW\":\n",
    "        optimizer = optim.AdamW(pair_net.parameters(), lr=0.00005, weight_decay=10.0)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "\n",
    "    epoch_score_lst = { phase: { pose: { category: [] for category in [\"ACC\",\"LOSS\"] } for pose in param.dataset_path_lst[phase].keys() } for phase in param.phase_lst }\n",
    "\n",
    "\n",
    "    for epoch in range(param.epoch_num):\n",
    "        print(\"----------------\")\n",
    "        print(\"epoch:\",epoch)\n",
    "        for phase in param.phase_lst:\n",
    "            print(\" phase:\",phase)\n",
    "            \n",
    "            if phase == \"Train\":\n",
    "                if separate_conf_mode:\n",
    "                    pair_net.model.eval()\n",
    "                    pair_net.model_conf.train()\n",
    "                else:\n",
    "                    pair_net.train()\n",
    "            else:\n",
    "                if separate_conf_mode:\n",
    "                    pair_net.model_conf.eval()\n",
    "                else:\n",
    "                    pair_net.eval()\n",
    "                        \n",
    "            for pose in param.dataset_path_lst[phase].keys():\n",
    "            \n",
    "                print(\"    pose:\",pose)\n",
    "\n",
    "                running_corrects = 0\n",
    "                total_input_size = 0\n",
    "                running_loss = 0\n",
    "                pred_lst = []\n",
    "                label_lst = []\n",
    "                path_0_lst = []\n",
    "                path_1_lst = []\n",
    "\n",
    "                for img_0_batch, img_1_batch, label_batch, path_0_batch, path_1_batch in dataloaders[phase][pose]:\n",
    "\n",
    "\n",
    "                    img_0_batch = img_0_batch.to(param.device)\n",
    "                    img_1_batch = img_1_batch.to(param.device)\n",
    "                    label_batch = label_batch.to(param.device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase==\"Train\"):\n",
    "\n",
    "                        if separate_conf_mode:\n",
    "                            x0_batch,x1_batch,conf0_batch,conf1_batch = pair_net.forward(img_0_batch,img_1_batch)\n",
    "                        else:\n",
    "                            if param.pair_net_conf_mode in [\"disabled\",\"integrated\"]:\n",
    "                                x0_batch,x1_batch,conf0_batch,conf1_batch = pair_net.forward(img_0_batch,img_1_batch)\n",
    "                            elif param.pair_net_conf_mode == \"separated\":\n",
    "                                x0_batch,x1_batch,conf0_batch,conf1_batch = pair_net.forward_only_pseudo(img_0_batch,img_1_batch)\n",
    "                            else:\n",
    "                                raise\n",
    "                            \n",
    "                        output_batch = torch.cat([x0_batch,x1_batch],dim=1)\n",
    "                        _, pred_batch = torch.max(output_batch, 1)\n",
    "\n",
    "                        if separate_conf_mode:\n",
    "                            loss = my_loss_func(x0_batch,x1_batch,conf0_batch,conf1_batch, label_batch, True, param )\n",
    "                        else:\n",
    "                            if param.pair_net_conf_mode in [\"disabled\",\"separated\"]: \n",
    "                                loss = my_loss_func(x0_batch,x1_batch,conf0_batch,conf1_batch, label_batch, False, param )\n",
    "                            elif param.pair_net_conf_mode == \"integrated\":\n",
    "                                loss = my_loss_func(x0_batch,x1_batch,conf0_batch,conf1_batch, label_batch, True, param )\n",
    "                            else:\n",
    "                                raise\n",
    "                            \n",
    "                        if phase == 'Train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        running_loss += loss.item() * img_0_batch.size(0)\n",
    "                        running_corrects += torch.sum(pred_batch == label_batch.data)\n",
    "                        total_input_size += img_0_batch.size(0)\n",
    "\n",
    "                    pred_lst += list(pred_batch.to('cpu').detach().numpy().copy())\n",
    "                    label_lst += list(label_batch.to('cpu').detach().numpy().copy())\n",
    "                    path_0_lst += list(path_0_batch)                       \n",
    "                    path_1_lst += list(path_1_batch)\n",
    "\n",
    "                   \n",
    "                epoch_acc = float((running_corrects.double()  / total_input_size).to('cpu').detach().numpy().copy())\n",
    "                epoch_loss = running_loss / total_input_size\n",
    "                         \n",
    "\n",
    "                print(\"          epoch_acc\",epoch_acc)\n",
    "                print(\"          epoch_loss\",epoch_loss)\n",
    "\n",
    "                epoch_score_lst[phase][pose][\"ACC\"].append(epoch_acc)\n",
    "                epoch_score_lst[phase][pose][\"LOSS\"].append(epoch_loss)\n",
    "\n",
    "                fp = open(\"./log/pair_net_epoch_score_lst_%s_%s.json\" % (param.au_name, \"posesall\") ,\"w\")\n",
    "                json.dump( epoch_score_lst, fp )\n",
    "                fp.close()\n",
    "\n",
    "\n",
    "\n",
    "                fp_pred_result = open(\"./pred_result/pair_net_%s_epoch_%d_phase_%s_%s.csv\" % (param.au_name,epoch,phase,pose),\"w\")\n",
    "                fp_pred_result.write(\"path_0,path_1,label,pred\\n\")\n",
    "                for i in range(len(path_0_lst)):\n",
    "                    fp_pred_result.write(\"%s,%s,%d,%d\\n\" % (path_0_lst[i],path_1_lst[i],int(label_lst[i]),int(pred_lst[i])) ) \n",
    "                fp_pred_result.close()\n",
    "\n",
    "\n",
    "                if phase == \"Train\":\n",
    "                    torch.save(pair_net.state_dict(), \"./model/pair_net_%s_epoch_%d_%s.pt\" % (param.au_name, epoch, \"posesall\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08736ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntensityTuneDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        self.df = pd.read_csv(csv_file,header=None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name_0 = self.df.iloc[idx,0]\n",
    "        label = self.df.iloc[idx,1]\n",
    "       \n",
    "        img_0 = load_img( img_name_0 )\n",
    "\n",
    "        return img_0,label,img_name_0\n",
    "\n",
    "\n",
    "def get_dataloaders_for_intensity_tune( param ):\n",
    "\n",
    "    dataloaders_intensity_tune = {}\n",
    "    for phase in param.phase_lst:\n",
    "        dataloaders_intensity_tune[phase] = {}\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "            csv_filename = param.dataset_lst_dirname + \"/AU_INT_TUNE_%s_%s_%s.csv\" % ( param.au_name, phase, pose)\n",
    "\n",
    "            intensity_tune_dataset = IntensityTuneDataset(csv_filename)\n",
    "            dataloader = torch.utils.data.DataLoader(intensity_tune_dataset, batch_size=param.batch_size_int, shuffle=True, num_workers=1)\n",
    "\n",
    "            dataloaders_intensity_tune[phase][pose] = dataloader\n",
    "            \n",
    "    return dataloaders_intensity_tune\n",
    "\n",
    "def predict_pairwise_net( pair_net_best_filename, param ):\n",
    "\n",
    "    \n",
    "    dataloaders_intensity_tune = get_dataloaders_for_intensity_tune( param )\n",
    "    \n",
    "    pair_net = PairNet( param )\n",
    "    pair_net.load_state_dict(torch.load(pair_net_best_filename))\n",
    "    pair_net.eval()\n",
    "    pair_net.to(param.device)\n",
    "\n",
    "    for phase in param.phase_lst:\n",
    "\n",
    "        print(\"------------\")\n",
    "        print(param.au_name,phase)\n",
    "\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "            print(\"phase\",phase,\"pose\",pose)\n",
    "\n",
    "            pred_lst_pose = []\n",
    "            label_lst_pose = []\n",
    "            path_lst_pose = []\n",
    "            conf_lst_pose = []\n",
    "\n",
    "            for img_0_batch, label_batch, path_batch in dataloaders_intensity_tune[phase][pose]:\n",
    "\n",
    "                img_0_batch = img_0_batch.to(param.device)\n",
    "                with torch.set_grad_enabled(False):\n",
    "\n",
    "                    x0_batch, conf0_batch = pair_net.forward_single(img_0_batch)\n",
    "\n",
    "                pred_batch = x0_batch.to('cpu').detach().numpy().copy()\n",
    "                pred_lst_pose += list(pred_batch)\n",
    "                label_lst_pose += list(label_batch)\n",
    "                path_lst_pose += list(path_batch)\n",
    "                \n",
    "                if param.pair_net_conf_mode != \"disabled\":\n",
    "                    conf0_batch = conf0_batch.to('cpu').detach().numpy().copy()\n",
    "                    conf_lst_pose += list(conf0_batch)\n",
    "\n",
    "\n",
    "            fp_pred_result_raw = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo.csv\" % (param.au_name,pose,phase),\"w\")\n",
    "            \n",
    "            if param.pair_net_conf_mode != \"disabled\":\n",
    "                fp_pred_result_raw.write(\"path,label,pred,conf\\n\")\n",
    "\n",
    "                for i in range(len(path_lst_pose)):\n",
    "                    fp_pred_result_raw.write(\"%s,%d,%f,%f\\n\"%(path_lst_pose[i],label_lst_pose[i],pred_lst_pose[i],conf_lst_pose[i]))\n",
    "\n",
    "            else:            \n",
    "                fp_pred_result_raw.write(\"path,label,pred\\n\")\n",
    "\n",
    "                for i in range(len(path_lst_pose)):\n",
    "                    fp_pred_result_raw.write(\"%s,%d,%f\\n\"%(path_lst_pose[i],label_lst_pose[i],pred_lst_pose[i]))\n",
    "\n",
    "            \n",
    "            \n",
    "            fp_pred_result_raw.close()            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7ba77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_for_intensity_net( phase_lst, param ):\n",
    "\n",
    "    for phase in phase_lst:\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "            phase_tmp = phase\n",
    "            if phase_tmp == \"TrainFull\":\n",
    "                phase_tmp = \"Train\"\n",
    "            if phase_tmp == \"ValidFull\":\n",
    "                phase_tmp = \"Valid\"\n",
    "            if phase_tmp == \"TestFull\":\n",
    "                phase_tmp = \"Test\"\n",
    "\n",
    "            csv_filename = \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo.csv\" % (param.au_name,pose,phase_tmp)\n",
    "\n",
    "            dataset_lst = { dataset_name:[os.path.abspath(dataset_path) for dataset_path in _dataset_path_lst] for dataset_name, _dataset_path_lst in param.intensity_tune_dataset_path_lst[phase][pose].items()}\n",
    "            dataset_name_lst = list(dataset_lst.keys())\n",
    "            \n",
    "\n",
    "            path_lst = { dataset_name: [] for dataset_name in dataset_name_lst }\n",
    "            label_lst = { dataset_name: [] for dataset_name in dataset_name_lst }\n",
    "            pred_lst = { dataset_name: [] for dataset_name in dataset_name_lst }\n",
    "            subject_lst = { dataset_name: [] for dataset_name in dataset_name_lst }\n",
    "\n",
    "            for line_id,line in enumerate(open(csv_filename)):\n",
    "                if line_id == 0:\n",
    "                    continue\n",
    "\n",
    "                if param.pair_net_conf_mode != \"disabled\":\n",
    "                    path,label,pred,conf = line.rstrip().split(\",\")\n",
    "                    label = int(label)\n",
    "                    pred = (float(pred),float(conf))\n",
    "                else:\n",
    "                    path,label,pred = line.rstrip().split(\",\")\n",
    "                    label = int(label)\n",
    "                    pred = float(pred)\n",
    "                \n",
    "                \n",
    "                subject = os.path.basename(path).split(\"_\")[2]\n",
    "\n",
    "\n",
    "\n",
    "                if len(dataset_name_lst) != 1:\n",
    "                    raise\n",
    "                dataset_name = dataset_name_lst[0]\n",
    "\n",
    "                path_lst[dataset_name].append(path)\n",
    "                label_lst[dataset_name].append(label)\n",
    "                pred_lst[dataset_name].append(pred)\n",
    "                subject_lst[dataset_name].append(subject)\n",
    "\n",
    "\n",
    "            output_csv_filename = param.dataset_lst_dirname + \"/AU_INT_CONV_%s_%s_%s.csv\" % (param.au_name,phase,pose)\n",
    "\n",
    "            if param.intensity_net_dataset_train_balance and phase == \"Train\":\n",
    "\n",
    "                filename_lst_cache_lst = {}\n",
    "\n",
    "                for dataset_name in dataset_lst.keys():\n",
    "\n",
    "                    filename_lst_cache = {}\n",
    "\n",
    "                    for i in range(len(path_lst[dataset_name])):\n",
    "\n",
    "                        subject = subject_lst[dataset_name][i]\n",
    "                        label = label_lst[dataset_name][i]\n",
    "\n",
    "                        if not label in filename_lst_cache:\n",
    "                            filename_lst_cache[label] = {}\n",
    "\n",
    "                        if not subject in filename_lst_cache[label]:\n",
    "                            filename_lst_cache[label][subject] = []\n",
    "\n",
    "                        filename_lst_cache[label][subject].append( (path_lst[dataset_name][i], pred_lst[dataset_name][i]) )\n",
    "\n",
    "                    filename_lst_cache_lst[dataset_name] = filename_lst_cache\n",
    "\n",
    "\n",
    "                fp = open(output_csv_filename,\"w\")\n",
    "\n",
    "                counter = 0\n",
    "                while counter < param.intensity_conv_dataset_num_lst[phase]:\n",
    "\n",
    "                    dataset_name = random.sample(dataset_name_lst,1)[0]\n",
    "                    filename_lst_cache = filename_lst_cache_lst[dataset_name]\n",
    "                    if len(list(filename_lst_cache.keys())) == 0:\n",
    "                        continue\n",
    "\n",
    "                    intensity = random.sample(list(filename_lst_cache.keys()),1)[0]                    \n",
    "\n",
    "                    subject = random.sample(sorted(list(filename_lst_cache[intensity].keys())),1)[0]\n",
    "\n",
    "                    path, pred = random.sample(filename_lst_cache[intensity][subject],1)[0]\n",
    "\n",
    "                    label = intensity\n",
    "\n",
    "                    if param.pair_net_conf_mode != \"disabled\":\n",
    "                        fp.write(\"%s,%d,%f,%f\\n\" % (path,label,pred[0],pred[1]))\n",
    "                    else:\n",
    "                        fp.write(\"%s,%d,%f\\n\" % (path,label,pred))\n",
    "                        \n",
    "                    counter += 1\n",
    "\n",
    "\n",
    "                fp.close()\n",
    "\n",
    "\n",
    "            else:\n",
    "\n",
    "                fp = open(output_csv_filename,\"w\")\n",
    "                print(csv_filename,len(path_lst))\n",
    "\n",
    "                for dataset_name in dataset_name_lst:\n",
    "\n",
    "                    n = param.intensity_conv_dataset_num_lst[phase] if param.intensity_conv_dataset_num_lst[phase] >= 0 else len(path_lst[dataset_name])\n",
    "                    \n",
    "                    print(param.au_name,phase,pose,dataset_name,\"sampling:\",n)\n",
    "\n",
    "\n",
    "                    if n <= len(path_lst[dataset_name]):\n",
    "                        rs_lst = random.sample(list(range(len(path_lst[dataset_name]))),n)\n",
    "                    else:\n",
    "                        rs_lst = random.choices(list(range(len(path_lst[dataset_name]))),k=n)\n",
    "\n",
    "                    for i in rs_lst:\n",
    "                        if param.pair_net_conf_mode != \"disabled\":\n",
    "                            fp.write(\"%s,%d,%f,%f\\n\" % (path_lst[dataset_name][i],label_lst[dataset_name][i],pred_lst[dataset_name][i][0],pred_lst[dataset_name][i][1]))\n",
    "                        else:\n",
    "                            fp.write(\"%s,%d,%f\\n\" % (path_lst[dataset_name][i],label_lst[dataset_name][i],pred_lst[dataset_name][i]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                fp.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e3305",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_pred_min_pred_max( param ):\n",
    "    \n",
    "    intensity_net_param_lst = {}\n",
    "        \n",
    "    pred_min = 100000\n",
    "    pred_max = -100000\n",
    "    \n",
    "    conf_min = 100000\n",
    "    conf_max = -100000\n",
    "\n",
    "    for phase in [\"Train\"]:\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "            csv_filename_all =\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo.csv\" % (param.au_name,pose,phase)\n",
    "\n",
    "            for line_id,line in enumerate(open(csv_filename_all)):\n",
    "\n",
    "                if line_id == 0:\n",
    "                    continue\n",
    "\n",
    "                pred = line.rstrip().split(\",\")[2]\n",
    "                pred = float(pred)\n",
    "\n",
    "                if pred_min > pred:\n",
    "                    pred_min = pred\n",
    "                if pred_max < pred:\n",
    "                    pred_max = pred\n",
    "                    \n",
    "                if param.pair_net_conf_mode != \"disabled\":\n",
    "                    conf = line.rstrip().split(\",\")[3]\n",
    "                    conf = float(conf)\n",
    "\n",
    "                    if conf_min > conf:\n",
    "                        conf_min = conf\n",
    "                    if conf_max < conf:\n",
    "                        conf_max = conf                    \n",
    "                    \n",
    "\n",
    "    print(pred_min,pred_max)\n",
    "    intensity_net_param = { \"pred_min\":pred_min, \"pred_max\":pred_max,\n",
    "                            \"conf_min\":conf_min, \"conf_max\":conf_max }\n",
    "\n",
    "        \n",
    "    return intensity_net_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e7d3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders_for_intensity_net( intensity_net_param, phase_lst, param ):\n",
    "\n",
    "    pred_min = intensity_net_param[\"pred_min\"]\n",
    "    pred_max = intensity_net_param[\"pred_max\"]\n",
    "    conf_min = intensity_net_param[\"conf_min\"]\n",
    "    conf_max = intensity_net_param[\"conf_max\"]\n",
    "    dataloaders_intensity_conv = {}\n",
    "    for phase in phase_lst:\n",
    "        dataloaders_intensity_conv[phase] = {}\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "\n",
    "            print(\"=========\")\n",
    "            print(phase,pose)\n",
    "\n",
    "            phase_tmp = phase\n",
    "            if phase_tmp == \"TrainFull\":\n",
    "                phase_tmp = \"Train\"\n",
    "            if phase_tmp == \"ValidFull\":\n",
    "                phase_tmp = \"Valid\"\n",
    "            if phase_tmp == \"TestFull\":\n",
    "                phase_tmp = \"Test\"\n",
    "\n",
    "            csv_filename_all =\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo.csv\" % (param.au_name,pose, phase_tmp)\n",
    "            csv_filename_target =param.dataset_lst_dirname + \"/AU_INT_CONV_%s_%s_%s.csv\" % (param.au_name,phase,pose)\n",
    "\n",
    "            intensity_conv_dataset = IntensityConvDataset(csv_filename_all,csv_filename_target,pred_min,pred_max,conf_min,conf_max,param)\n",
    "            dataloader = torch.utils.data.DataLoader(intensity_conv_dataset, batch_size=param.batch_size_int, shuffle=True, num_workers=1 )\n",
    "\n",
    "            dataloaders_intensity_conv[phase][pose] = dataloader\n",
    "\n",
    "    return dataloaders_intensity_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d283d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_by_min_max( x, pred_min, pred_max ):\n",
    "    return (x - pred_min) / (pred_max-pred_min) * 2 - 1\n",
    "\n",
    "def normalize_by_min_max_conf( x, conf_min, conf_max ):\n",
    "    conf = 1-((x - conf_min) / (conf_max-conf_min))\n",
    "    conf = min(max(conf,0),1)\n",
    "    return conf\n",
    "    \n",
    "    \n",
    "\n",
    "def get_percentile_feature( pred_lst, param ):\n",
    "    percentile_feature = []\n",
    "    \n",
    "    pred_lst_sorted = sorted(pred_lst)\n",
    "    \n",
    "    for p in param.intensity_net_percentile_th_lst:\n",
    "        percentile_feature.append(pred_lst_sorted[int((len(pred_lst_sorted)-1)*p)])\n",
    "\n",
    "    return percentile_feature\n",
    "\n",
    "def get_percentile_conf_feature( pred_lst, conf_lst, param ):\n",
    "    percentile_feature = []\n",
    "    prev_percentile_feature = None\n",
    "    \n",
    "    conf_pred_db = { th:[] for th in param.intensity_net_conf_th_lst[:-1] }\n",
    "    \n",
    "    for i in range(len(pred_lst)):\n",
    "        \n",
    "        pred = pred_lst[i]\n",
    "        conf = conf_lst[i]\n",
    "        \n",
    "        for th in conf_pred_db.keys():\n",
    "            if th <= conf <= param.intensity_net_conf_th_lst[-1]:\n",
    "                conf_pred_db[th].append(pred)\n",
    "    \n",
    "    for th in conf_pred_db.keys():\n",
    "        \n",
    "        _pred_lst = conf_pred_db[th]\n",
    "        \n",
    "        if len(_pred_lst) > 0:\n",
    "            prev_percentile_feature = get_percentile_feature( _pred_lst, param )\n",
    "   \n",
    "        if prev_percentile_feature is None:\n",
    "            raise\n",
    "            \n",
    "        percentile_feature += prev_percentile_feature\n",
    "        \n",
    "        percentile_feature += [ len(_pred_lst)/len(pred_lst) ]\n",
    "        \n",
    "\n",
    "    return percentile_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2bae27-993e-49b2-9c8c-376d26f21d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_window_feature(frame_id_to_info,frame_id,param):\n",
    "    \n",
    "    pred = frame_id_to_info[frame_id][0]\n",
    "    path = frame_id_to_info[frame_id][1]\n",
    "    conf = frame_id_to_info[frame_id][2]\n",
    "\n",
    "    feature = { \"percentile_window\":[], \"percentile_conf_window\":[] }\n",
    "\n",
    "    for ws in param.intensity_net_percentile_window_size_lst:\n",
    "        t_pseudo = [pred]\n",
    "        t_conf = [conf]\n",
    "        for j in range(1,1+ws):\n",
    "\n",
    "            if frame_id+j in frame_id_to_info:\n",
    "                t_pseudo.append(frame_id_to_info[frame_id+j][0])\n",
    "                t_conf.append(frame_id_to_info[frame_id+j][2])                                \n",
    "\n",
    "\n",
    "            if frame_id-j in frame_id_to_info:\n",
    "                t_pseudo.append(frame_id_to_info[frame_id-j][0])\n",
    "                t_conf.append(frame_id_to_info[frame_id-j][2])                                \n",
    "\n",
    "\n",
    "        percentile_feature = get_percentile_feature( t_pseudo, param )\n",
    "        feature[\"percentile_window\"] += percentile_feature\n",
    "\n",
    "        percentile_conf_feature = get_percentile_conf_feature( t_pseudo, t_conf, param )\n",
    "        feature[\"percentile_conf_window\"] += percentile_conf_feature\n",
    "        \n",
    "\n",
    "    ws = param.intensity_net_pseudo_window_size\n",
    "    t_pseudo_f = [pred]\n",
    "    t_conf_f = [conf]\n",
    "    t_pseudo_b = [pred]\n",
    "    t_conf_b = [conf]\n",
    "    for j in range(1,1+ws):\n",
    "\n",
    "        if frame_id+j in frame_id_to_info:\n",
    "            t_pseudo_f.append(frame_id_to_info[frame_id+j][0])\n",
    "            t_conf_f.append(frame_id_to_info[frame_id+j][2])                                \n",
    "        else:\n",
    "            t_pseudo_f += [t_pseudo_f[-1]]\n",
    "            t_conf_f += [t_conf_f[-1]]                            \n",
    "\n",
    "        if frame_id-j in frame_id_to_info:\n",
    "            t_pseudo_b.append(frame_id_to_info[frame_id-j][0])\n",
    "            t_conf_b.append(frame_id_to_info[frame_id-j][2])                                \n",
    "        else:\n",
    "            t_pseudo_b += [t_pseudo_b[-1]]\n",
    "            t_conf_b += [t_conf_b[-1]]  \n",
    "\n",
    "\n",
    "    feature[\"pseudo_window\"] = t_pseudo_f[1:] + t_pseudo_b[1:]\n",
    "    feature[\"conf_window\"] = t_conf_f[1:] + t_conf_b[1:]\n",
    "        \n",
    "        \n",
    "        \n",
    "    return feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60945532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntensityConvDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, csv_filename_all, csv_filename_target, pred_min, pred_max, conf_min, conf_max, param ):\n",
    "        \n",
    "        print(\"file:\",csv_filename_target)\n",
    "        \n",
    "        self.subject_feature_lst = {}\n",
    "        self.path_feature_lst = {}\n",
    "        self.param = param\n",
    "        \n",
    "        self.data_lst = []           \n",
    "        for line_id,line in enumerate(open(csv_filename_target)):          \n",
    "            \n",
    "            if param.pair_net_conf_mode != \"disabled\":\n",
    "                path,label,pred,conf = line.rstrip().split(\",\")\n",
    "                conf = float(conf)\n",
    "            else:\n",
    "                path,label,pred = line.rstrip().split(\",\")\n",
    "                conf = 0\n",
    "                \n",
    "            basename = os.path.splitext(os.path.basename(path))[0]\n",
    "\n",
    "            subject = basename.split(\"_\")[2]\n",
    "            task = basename.split(\"_\")[3]\n",
    "            pose = basename.split(\"_\")[4]\n",
    "            frame_id = int(basename.split(\"_\")[5][len(\"frame\"):])\n",
    "\n",
    "            label = int(label)\n",
    "            pred = float(pred)\n",
    "            \n",
    "            conf = normalize_by_min_max_conf( conf, 0, conf_max )\n",
    "            pred = normalize_by_min_max( pred, pred_min, pred_max )\n",
    "            \n",
    "            self.data_lst.append( (path,subject,pose,task,pred,label,conf,frame_id) )\n",
    "    \n",
    "        for line_id,line in enumerate(open(csv_filename_all)):\n",
    "\n",
    "            if line_id == 0:\n",
    "                continue\n",
    "\n",
    "            if param.pair_net_conf_mode != \"disabled\":\n",
    "                path,label,pred,conf = line.rstrip().split(\",\")\n",
    "                conf = float(conf)\n",
    "            else:\n",
    "                path,label,pred = line.rstrip().split(\",\")\n",
    "                conf = 0\n",
    "\n",
    "            subject = os.path.basename(path).split(\"_\")[2]\n",
    "            task = os.path.basename(path).split(\"_\")[3]\n",
    "            pose = os.path.basename(path).split(\"_\")[4]            \n",
    "\n",
    "            label = int(label)\n",
    "            pred = float(pred)\n",
    "            \n",
    "            conf = normalize_by_min_max_conf( conf, 0, conf_max )\n",
    "            pred = normalize_by_min_max( pred, pred_min, pred_max )\n",
    "            \n",
    "            \n",
    "            if not subject in self.subject_feature_lst:\n",
    "                self.subject_feature_lst[subject] = {}\n",
    "            if not pose in self.subject_feature_lst[subject]:\n",
    "                self.subject_feature_lst[subject][pose] = {}\n",
    "            if not task in self.subject_feature_lst[subject][pose]:\n",
    "                self.subject_feature_lst[subject][pose][task] = { \"pred_lst\":[], \"basename_lst\":[], \"path_lst\":[], \"frame_id_lst\":[], \"conf_lst\":[] }\n",
    "                \n",
    "            self.subject_feature_lst[subject][pose][task][\"pred_lst\"].append(pred)\n",
    "            self.subject_feature_lst[subject][pose][task][\"path_lst\"].append(path)\n",
    "            self.subject_feature_lst[subject][pose][task][\"conf_lst\"].append(conf)\n",
    "            \n",
    "            \n",
    "            basename = os.path.splitext(os.path.basename(path))[0]\n",
    "            self.subject_feature_lst[subject][pose][task][\"basename_lst\"].append(basename)\n",
    "            frame_id = int(basename.split(\"_\")[5][len(\"frame\"):])\n",
    "            self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"].append(frame_id)\n",
    "\n",
    "        for subject in self.subject_feature_lst.keys():\n",
    "            for pose in self.subject_feature_lst[subject].keys():\n",
    "                for task in self.subject_feature_lst[subject][pose].keys():   \n",
    "                    \n",
    "                    if len(self.subject_feature_lst[subject][pose][task][\"pred_lst\"]) <= 1:\n",
    "                        print(\"[WARN] no frame\",\n",
    "                              len(self.subject_feature_lst[subject][pose][task][\"pred_lst\"]),\n",
    "                              self.subject_feature_lst[subject][pose][task][\"pred_lst\"])\n",
    "                        \n",
    "                        self.subject_feature_lst[subject][pose][task][\"percentile\"] = [0.0]*len(param.intensity_net_percentile_th_lst)\n",
    "                        \n",
    "                        continue\n",
    "                        \n",
    "                  \n",
    "                    self.subject_feature_lst[subject][pose][task][\"percentile\"] = get_percentile_feature( self.subject_feature_lst[subject][pose][task][\"pred_lst\"], param )\n",
    "                    \n",
    "                    self.subject_feature_lst[subject][pose][task][\"percentile_conf\"] = get_percentile_conf_feature( self.subject_feature_lst[subject][pose][task][\"pred_lst\"], self.subject_feature_lst[subject][pose][task][\"conf_lst\"], param )\n",
    "             \n",
    "                    \n",
    "                    frame_id_lst = self.subject_feature_lst[subject][pose][task][\"frame_id_lst\"]\n",
    "                    pred_lst = self.subject_feature_lst[subject][pose][task][\"pred_lst\"]\n",
    "                    path_lst = self.subject_feature_lst[subject][pose][task][\"path_lst\"]\n",
    "                    conf_lst = self.subject_feature_lst[subject][pose][task][\"conf_lst\"]\n",
    "                    \n",
    "                    frame_id_to_info = {}\n",
    "                    for i in range(len(frame_id_lst)):\n",
    "                        frame_id = frame_id_lst[i]\n",
    "                        pred = pred_lst[i]\n",
    "                        path = path_lst[i]\n",
    "                        conf = conf_lst[i]\n",
    "                        frame_id_to_info[ frame_id ] = (pred,path,conf)\n",
    "                        \n",
    "                    self.subject_feature_lst[subject][pose][task][\"frame_id_to_info\"] = frame_id_to_info\n",
    "\n",
    "\n",
    "        for subject in self.subject_feature_lst.keys():\n",
    "            for pose in self.subject_feature_lst[subject].keys():\n",
    "                \n",
    "                pred_lst = []\n",
    "                \n",
    "                for task in self.subject_feature_lst[subject][pose].keys():\n",
    "                    pred_lst += self.subject_feature_lst[subject][pose][task][\"pred_lst\"]\n",
    "                \n",
    "                if len(pred_lst) <= 1:\n",
    "                    percentile_lst = [0.0]*len(param.intensity_net_percentile_th_lst)\n",
    "                \n",
    "                else:                  \n",
    "                    percentile_lst = get_percentile_feature( pred_lst, param )                    \n",
    "\n",
    "                for task in self.subject_feature_lst[subject][pose].keys():\n",
    "                    self.subject_feature_lst[subject][pose][task][\"percentile__subject_pose\"] = percentile_lst\n",
    "                \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_lst)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path,subject,pose,task,psedo_pred,label,conf,frame_id = self.data_lst[idx]\n",
    "\n",
    "        window_feature = get_window_feature(self.subject_feature_lst[subject][pose][task][\"frame_id_to_info\"],\n",
    "                                            frame_id,self.param)\n",
    "        \n",
    "        \n",
    "        feature = []\n",
    "        if \"percentile\" in self.param.intensity_net_feature_lst:\n",
    "            feature += list(self.subject_feature_lst[subject][pose][task][\"percentile\"])            \n",
    "        if \"percentile__subject_pose\" in self.param.intensity_net_feature_lst:\n",
    "            feature += list(self.subject_feature_lst[subject][pose][task][\"percentile__subject_pose\"])\n",
    "        if \"percentile_conf\" in self.param.intensity_net_feature_lst:\n",
    "            feature += list(self.subject_feature_lst[subject][pose][task][\"percentile_conf\"])\n",
    "        if \"percentile_window\" in self.param.intensity_net_feature_lst:            \n",
    "            feature += list(window_feature[\"percentile_window\"])            \n",
    "        if \"percentile_conf_window\" in self.param.intensity_net_feature_lst:            \n",
    "            feature += list(window_feature[\"percentile_conf_window\"])\n",
    "        if \"pseudo_window\" in self.param.intensity_net_feature_lst:            \n",
    "            feature += list(window_feature[\"pseudo_window\"])            \n",
    "        if \"conf_window\" in self.param.intensity_net_feature_lst:            \n",
    "            feature += list(window_feature[\"conf_window\"])\n",
    "        if \"conf\" in self.param.intensity_net_feature_lst:\n",
    "            feature += [conf]\n",
    "\n",
    "        \n",
    "        return torch.tensor(feature), \\\n",
    "               torch.tensor([psedo_pred]), \\\n",
    "               torch.tensor(label), \\\n",
    "               path\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab2747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntensityNet(nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(IntensityNet, self).__init__()\n",
    "        \n",
    "        classifier_in_dim = 1\n",
    "\n",
    "        if \"percentile\" in param.intensity_net_feature_lst:\n",
    "            classifier_in_dim += len(param.intensity_net_percentile_th_lst)\n",
    "        if \"percentile__subject_pose\" in param.intensity_net_feature_lst:\n",
    "            classifier_in_dim += len(param.intensity_net_percentile_th_lst)\n",
    "        if \"percentile_conf\" in param.intensity_net_feature_lst:\n",
    "            classifier_in_dim += (len(param.intensity_net_percentile_th_lst)+1) * (len(param.intensity_net_conf_th_lst)-1)\n",
    "        if \"percentile_window\" in param.intensity_net_feature_lst:\n",
    "            classifier_in_dim += len(param.intensity_net_percentile_th_lst) * len(param.intensity_net_percentile_window_size_lst)\n",
    "        if \"percentile_conf_window\" in param.intensity_net_feature_lst:\n",
    "            classifier_in_dim += ((len(param.intensity_net_percentile_th_lst)+1) * (len(param.intensity_net_conf_th_lst)-1)) * len(param.intensity_net_percentile_window_size_lst)\n",
    "        if \"pseudo_window\" in param.intensity_net_feature_lst:            \n",
    "            classifier_in_dim += param.intensity_net_pseudo_window_size*2          \n",
    "        if \"conf_window\" in param.intensity_net_feature_lst:            \n",
    "            classifier_in_dim += param.intensity_net_pseudo_window_size*2\n",
    "        if \"conf\" in param.intensity_net_feature_lst:\n",
    "            classifier_in_dim += 1\n",
    "\n",
    "        classifier_out_dim = 1\n",
    "        \n",
    "            \n",
    "            \n",
    "        self.layers_classifier = []\n",
    "        self.layers_classifier.append(nn.Linear(in_features=classifier_in_dim, out_features=4096, bias=True))      \n",
    "        self.layers_classifier.append(nn.ReLU(inplace=True))\n",
    "        self.layers_classifier.append(nn.Dropout(p=0.5, inplace=False))\n",
    "        self.layers_classifier.append(nn.Linear(in_features=4096, out_features=4096, bias=True))\n",
    "        self.layers_classifier.append(nn.ReLU(inplace=True))\n",
    "        self.layers_classifier.append(nn.Dropout(p=0.5, inplace=False))        \n",
    "        self.layers_classifier.append(nn.Linear(in_features=4096, out_features=classifier_out_dim, bias=True))\n",
    "        self.layers_classifier = nn.ModuleList(self.layers_classifier)\n",
    "        \n",
    "\n",
    "    def forward(self, x, feature ):\n",
    "\n",
    "        feature = feature.float()\n",
    "              \n",
    "        x = torch.cat([x,feature],dim=1)\n",
    "\n",
    "        for i in range(len(self.layers_classifier)):\n",
    "            x = self.layers_classifier[i](x)\n",
    "            \n",
    "            \n",
    "            \n",
    "        conf = None\n",
    "            \n",
    "        \n",
    "        return x,conf\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07387261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_intensity_net( intensity_net_param, param ):\n",
    "\n",
    "    dataloaders_intensity_conv = get_dataloaders_for_intensity_net( intensity_net_param, param.phase_lst, param )    \n",
    "    \n",
    "    \n",
    "    intensity_net = IntensityNet(param)\n",
    "    intensity_net.to(param.device)\n",
    "\n",
    "    criterion = nn.L1Loss()\n",
    "\n",
    "    if param.intensity_net_optimizer == \"Adam\":\n",
    "        optimizer = optim.Adam(intensity_net.parameters(), lr=0.00005)\n",
    "    elif param.intensity_net_optimizer == \"AdamW\":\n",
    "        optimizer = optim.AdamW(intensity_net.parameters(), lr=0.00005, weight_decay=1.0)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    epoch_score_lst = { phase:{ pose:{ param.measurement_name:[], \"LOSS\":[] } for pose in param.intensity_tune_dataset_path_lst[phase].keys() } for phase in param.phase_lst }\n",
    "\n",
    "    for epoch in range(param.epoch_num):\n",
    "        print(\"================\")\n",
    "        print(\"epoch\",epoch)\n",
    "\n",
    "        for phase in param.phase_lst:\n",
    "\n",
    "            if phase == \"Train\":\n",
    "                intensity_net.train()\n",
    "            else:\n",
    "                intensity_net.eval()         \n",
    "\n",
    "            print(\"------------\")\n",
    "            print(param.au_name,phase)\n",
    "\n",
    "            pred_lst = []\n",
    "            label_lst = []\n",
    "            path_lst = []\n",
    "\n",
    "            icc_lst = []\n",
    "\n",
    "            for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "                print(\"phase\",phase,\"pose\",pose)\n",
    "\n",
    "                pred_lst_pose = []\n",
    "                psedo_pred_lst_pose = []\n",
    "                label_lst_pose = []\n",
    "                path_lst_pose = []\n",
    "\n",
    "                running_loss = 0\n",
    "                running_corrects = 0\n",
    "                total_input_size = 0\n",
    "\n",
    "\n",
    "                for feature_batch, psedo_pred_batch, label_batch, path_batch in dataloaders_intensity_conv[phase][pose]:\n",
    "\n",
    "                    feature_batch = feature_batch.to(param.device)\n",
    "                    psedo_pred_batch = psedo_pred_batch.to(param.device)\n",
    "                    label_batch_org = label_batch\n",
    "                    label_batch = label_batch.to(param.device)\n",
    "\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    with torch.set_grad_enabled(phase == 'Train'):\n",
    "\n",
    "                        output_batch, conf_batch = intensity_net.forward(psedo_pred_batch,feature_batch)                           \n",
    "                        pred_batch = torch.flatten(torch.round(torch.clamp( output_batch, min=0, max=(param.intensity_level-1) )).long())\n",
    "\n",
    "\n",
    "                        loss = criterion(torch.flatten(output_batch), label_batch.float())\n",
    "\n",
    "                        if phase == 'Train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                        running_loss += loss.item() * feature_batch.size(0)\n",
    "                        running_corrects += torch.sum(pred_batch == label_batch.data)\n",
    "                        total_input_size += feature_batch.size(0)\n",
    "\n",
    "                    pred_lst_pose += list(pred_batch.to('cpu').detach().numpy().copy())\n",
    "                    psedo_pred_lst_pose += list(psedo_pred_batch.to('cpu').detach().numpy().copy())\n",
    "                    label_lst_pose += list(label_batch_org.numpy().copy())\n",
    "                    path_lst_pose += list(path_batch)\n",
    "\n",
    "                cur_icc = get_measurement_score(label_lst_pose,pred_lst_pose,param.int_occ_mode)\n",
    "                print(param.measurement_name,cur_icc)\n",
    "                icc_lst.append(cur_icc)\n",
    "\n",
    "                loss_pose = running_loss / total_input_size\n",
    "\n",
    "                pred_lst += pred_lst_pose\n",
    "                label_lst += label_lst_pose\n",
    "                path_lst += path_lst_pose\n",
    "\n",
    "                epoch_score_lst[phase][pose][param.measurement_name].append(cur_icc)\n",
    "                epoch_score_lst[phase][pose][\"LOSS\"].append(loss_pose)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            print(param.measurement_name,\"(MEAN)\",np.average(icc_lst))\n",
    "\n",
    "\n",
    "            fp = open(\"./log/intensity_net_epoch_score_lst_%s_%s.json\" % (param.au_name, \"posesall\") ,\"w\")\n",
    "            json.dump( epoch_score_lst, fp )\n",
    "            fp.close()\n",
    "\n",
    "\n",
    "            if phase == \"Train\":\n",
    "                torch.save(intensity_net.state_dict(), \"./model/intensity_net_%s_epoch_%d_%s.pt\" % (param.au_name, epoch, \"posesall\" ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae307b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intensity_net_best_epoch( param ):\n",
    "\n",
    "    print(\"=============\")\n",
    "    print(param.au_name)\n",
    "\n",
    "    filename = \"./log/intensity_net_epoch_score_lst_%s_%s.json\" % (param.au_name,\"posesall\")\n",
    "\n",
    "    intensity_net_best_epoch = get_best_epoch( filename, \"LOSS\", \"LOWER\", param )\n",
    "    show_score_lst( filename, param.dataset_path_lst )\n",
    "    \n",
    "    return intensity_net_best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002c4467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_intensity_net( dataloaders_intensity_conv, intensity_net_best_filename, phase_lst, param ):\n",
    "\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    intensity_net = IntensityNet( param )\n",
    "    intensity_net.load_state_dict(torch.load(intensity_net_best_filename))\n",
    "    intensity_net.eval()\n",
    "    intensity_net.to(param.device)\n",
    "\n",
    "    for phase in phase_lst:\n",
    "\n",
    "        print(\"------------\")\n",
    "        print(param.au_name,phase)\n",
    "\n",
    "        icc_lst = {}\n",
    "\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "            print(\"phase\",phase,\"pose\",pose)\n",
    "\n",
    "            pred_lst_pose = []\n",
    "            pred_raw_lst_pose = []\n",
    "            psedo_pred_lst_pose = []\n",
    "            label_lst_pose = []\n",
    "            path_lst_pose = []\n",
    "            pred_prob_lst_pose = []\n",
    "            conf_lst_pose = []\n",
    "\n",
    "            running_corrects = 0\n",
    "            total_input_size = 0\n",
    "\n",
    "\n",
    "            for feature_batch, psedo_pred_batch, label_batch, path_batch in dataloaders_intensity_conv[phase][pose]:\n",
    "\n",
    "                feature_batch = feature_batch.to(param.device)\n",
    "                psedo_pred_batch = psedo_pred_batch.to(param.device)\n",
    "                label_batch_org = label_batch\n",
    "                label_batch = label_batch.to(param.device)\n",
    "\n",
    "\n",
    "                with torch.set_grad_enabled(False):\n",
    "\n",
    "                    output_batch, conf_batch = intensity_net.forward(psedo_pred_batch,feature_batch)\n",
    "                    pred_batch = torch.flatten(torch.round(torch.clamp( output_batch, min=0, max=(param.intensity_level-1) )).long())\n",
    "                    pred_raw_batch = torch.flatten(torch.clamp( output_batch, min=0, max=(param.intensity_level-1) ))\n",
    "\n",
    "\n",
    "                    running_corrects += torch.sum(pred_batch == label_batch.data)\n",
    "                    total_input_size += feature_batch.size(0)\n",
    "\n",
    "                pred_lst_pose += list(pred_batch.to('cpu').detach().numpy().copy())\n",
    "                pred_raw_lst_pose += list(pred_raw_batch.to('cpu').detach().numpy().copy())\n",
    "                psedo_pred_lst_pose += list(psedo_pred_batch.to('cpu').detach().numpy().copy())\n",
    "                label_lst_pose += list(label_batch_org.numpy().copy())\n",
    "                path_lst_pose += list(path_batch)\n",
    "\n",
    "\n",
    "            cur_icc = get_measurement_score(label_lst_pose,pred_lst_pose,param.int_occ_mode)\n",
    "            print(param.measurement_name,cur_icc)\n",
    "            print(\"len\",len(label_lst_pose))\n",
    "            icc_lst[pose] = cur_icc\n",
    "\n",
    "\n",
    "            fp_pred_result = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s.csv\" % (param.au_name,pose,phase),\"w\")\n",
    "            \n",
    "\n",
    "            fp_pred_result.write(\"path,label,pred\\n\")\n",
    "\n",
    "            for i in range(len(path_lst_pose)):\n",
    "                fp_pred_result.write(\"%s,%d,%d\\n\"%(path_lst_pose[i],label_lst_pose[i],pred_lst_pose[i]))\n",
    "\n",
    "                    \n",
    "            fp_pred_result.close()\n",
    "\n",
    "            fp_pred_result = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_raw.csv\" % (param.au_name,pose,phase),\"w\")\n",
    "            \n",
    "\n",
    "            fp_pred_result.write(\"path,label,pred\\n\")\n",
    "\n",
    "            for i in range(len(path_lst_pose)):\n",
    "                fp_pred_result.write(\"%s,%d,%f\\n\"%(path_lst_pose[i],label_lst_pose[i],pred_raw_lst_pose[i]))\n",
    "\n",
    "            fp_pred_result.close()\n",
    "            \n",
    "            fp_pred_result_raw = open(\"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo_norm.csv\" % (param.au_name,pose,phase),\"w\")\n",
    "            fp_pred_result_raw.write(\"path,label,pred\\n\")\n",
    "\n",
    "            for i in range(len(path_lst_pose)):\n",
    "                fp_pred_result_raw.write(\"%s,%d,%f\\n\"%(path_lst_pose[i],label_lst_pose[i],psedo_pred_lst_pose[i]))\n",
    "            fp_pred_result_raw.close()\n",
    "\n",
    "\n",
    "\n",
    "        print(param.measurement_name,\"(MEAN)\",np.average(list(icc_lst.values())))\n",
    "\n",
    "        fp = open(\"./log/score_result_%s_%s.csv\"%(param.au_name,phase),\"w\")\n",
    "        fp.write(\"pose,%s\\n\"%param.measurement_name)\n",
    "        for pose in sorted(icc_lst.keys()):\n",
    "            cur_icc = icc_lst[pose]\n",
    "            fp.write(\"%s,%f\\n\"%(pose,cur_icc))\n",
    "        fp.write(\"pose-mean,%f\\n\"%np.average(list(icc_lst.values())))           \n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2b4a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_training_trial( trial, param ):\n",
    "    \n",
    "    if param.only_intensity_net_process:\n",
    "        \n",
    "        pairwise_net_best_epoch = None\n",
    "        \n",
    "        pair_net_best_filename = \"./model/pair_net_%s_epoch_best_%s_trial_%d.pt\" % (param.au_name, \"posesall\", trial )\n",
    "        \n",
    "        for phase in param.phase_lst:\n",
    "            for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "                shutil.copyfile( \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo_trial_%d.csv\" % (param.au_name,pose,phase,trial ),\n",
    "                                 \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo.csv\" % (param.au_name,pose,phase ))\n",
    "    else:\n",
    "        \n",
    "        train_pairwise_net( param, False )    \n",
    "\n",
    "        pairwise_net_best_epoch = get_pairwise_net_best_epoch( param )\n",
    "\n",
    "        if param.pair_net_conf_mode == \"separated\":\n",
    "            train_pairwise_net( param, True, \"./model/pair_net_%s_epoch_%d_%s.pt\" % (param.au_name, pairwise_net_best_epoch, \"posesall\" ) )\n",
    "            pairwise_net_best_epoch = get_pairwise_net_best_epoch( param )\n",
    "\n",
    "        shutil.copyfile( \"./model/pair_net_%s_epoch_%d_%s.pt\" % (param.au_name, pairwise_net_best_epoch, \"posesall\" ),\n",
    "                         \"./model/pair_net_%s_epoch_best_%s_trial_%d.pt\" % (param.au_name, \"posesall\", trial ))\n",
    "        pair_net_best_filename = \"./model/pair_net_%s_epoch_best_%s_trial_%d.pt\" % (param.au_name, \"posesall\", trial )\n",
    "\n",
    "\n",
    "        predict_pairwise_net( pair_net_best_filename, param )\n",
    "\n",
    "\n",
    "        for phase in param.phase_lst:\n",
    "            for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "                shutil.copyfile( \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo.csv\" % (param.au_name,pose,phase ),\n",
    "                                 \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo_trial_%d.csv\" % (param.au_name,pose,phase,trial ) )\n",
    "\n",
    "    make_dataset_for_intensity_net( param.phase_lst, param )\n",
    "    intensity_net_param = get_pred_min_pred_max( param )\n",
    "    train_intensity_net( intensity_net_param, param )\n",
    "    intensity_net_best_epoch = get_intensity_net_best_epoch( param )\n",
    "    \n",
    "    fp = open(\"./model/intensity_net_param_lst_trial_%d.json\" % trial,\"w\")\n",
    "    json.dump( { param.au_name:intensity_net_param }, fp )\n",
    "    fp.close()\n",
    "    \n",
    "    shutil.copyfile( \"./model/intensity_net_%s_epoch_%d_%s.pt\" % (param.au_name, intensity_net_best_epoch,\"posesall\"),\n",
    "                     \"./model/intensity_net_%s_epoch_best_%s_trial_%d.pt\" % (param.au_name, \"posesall\", trial ))\n",
    "    \n",
    "\n",
    "    \n",
    "    return pairwise_net_best_epoch, intensity_net_best_epoch\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2fb73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_log( fp_summary, pair_net_epoch_score_lst, au_name, phase_lst, network_name, best_epoch, trial ):\n",
    "    \n",
    "    best_validation_score = None\n",
    "    \n",
    "    for phase in phase_lst:\n",
    "        measure_lst = list(pair_net_epoch_score_lst[phase].values())[0].keys()\n",
    "        for measure in measure_lst:\n",
    "            epoch_lst = range(len(list(list(pair_net_epoch_score_lst[phase].values())[0].values())[0]))\n",
    "            for epoch in epoch_lst:\n",
    "                pose_lst = list(pair_net_epoch_score_lst[phase].keys())\n",
    "                \n",
    "                best_epoch_flag = 1 if best_epoch == epoch else 0\n",
    "                \n",
    "                score_all_pose_lst = []\n",
    "                for pose in pose_lst:                    \n",
    "                    score = pair_net_epoch_score_lst[phase][pose][measure][epoch]\n",
    "                    score_all_pose_lst.append(score)\n",
    "                    \n",
    "                    fp_summary.write(\"%s,%d,%s,%s,%s,%d,%d,%s,%f\\n\"%(au_name,trial,network_name,phase,pose,epoch,best_epoch_flag,measure,score))\n",
    "\n",
    "                fp_summary.write(\"%s,%d,%s,%s,%s,%d,%d,%s,%f\\n\"%(au_name,trial,network_name,phase,\"pose-mean\",epoch,best_epoch_flag,measure,np.mean(score_all_pose_lst)))\n",
    "                \n",
    "                if phase == \"Valid\" and measure == \"LOSS\" and epoch == best_epoch:\n",
    "                    best_validation_score = np.mean(score_all_pose_lst)\n",
    "\n",
    "    fp_summary.flush()\n",
    "    \n",
    "    return best_validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647cf0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_training( param ):\n",
    "    \n",
    "    best_validation_loss_lst = []\n",
    "\n",
    "    fp_summary = open(\"./log/summary.csv\",\"w\")\n",
    "    fp_summary.write(\"au_name,trial,network_name,phase,pose,epoch,best_epoch_flag,measure,score\\n\")\n",
    "\n",
    "\n",
    "    for trial in range(param.trial_num):\n",
    "\n",
    "        print(\"@@@@@@@@@@@@@@@@@@@@@@@@@@@@\")\n",
    "        print(\"trial:\",trial)\n",
    "\n",
    "        pairwise_net_best_epoch, intensity_net_best_epoch = exec_training_trial( trial, param )\n",
    "\n",
    "    \n",
    "        if not(pairwise_net_best_epoch is None):\n",
    "            network_name = \"pairwise\"\n",
    "            best_epoch = pairwise_net_best_epoch\n",
    "            fp = open(\"./log/pair_net_epoch_score_lst_%s_posesall.json\" % param.au_name)\n",
    "            pair_net_epoch_score_lst = json.load( fp )\n",
    "            fp.close()\n",
    "            write_summary_log( fp_summary, pair_net_epoch_score_lst, param.au_name, param.phase_lst, network_name, best_epoch, trial )\n",
    "\n",
    "        network_name = \"intensity\"\n",
    "        best_epoch = intensity_net_best_epoch\n",
    "        fp = open(\"./log/intensity_net_epoch_score_lst_%s_posesall.json\" % param.au_name)\n",
    "        intensity_net_epoch_score_lst = json.load( fp )\n",
    "        fp.close()\n",
    "        best_validation_loss = write_summary_log( fp_summary, intensity_net_epoch_score_lst, param.au_name, param.phase_lst, network_name, best_epoch, trial )\n",
    "\n",
    "        best_validation_loss_lst.append( best_validation_loss )\n",
    "\n",
    "    if \"Valid\" in param.phase_lst:\n",
    "        best_trial = np.argmin(best_validation_loss_lst)\n",
    "    else:\n",
    "        best_trial = 0\n",
    "    \n",
    "    print(\"best_trial:\",best_trial)\n",
    "    \n",
    "    return best_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ee0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_prediction( best_trial, param ):\n",
    "    \n",
    "    for _phase in param.phase_full_lst:\n",
    "        phase = _phase.replace(\"Full\",\"\")\n",
    "        for pose in param.intensity_tune_dataset_path_lst[phase].keys():\n",
    "            shutil.copyfile( \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo_trial_%d.csv\" % (param.au_name,pose,phase,best_trial),\n",
    "                             \"./pred_result/pred_result__%s_procrustes-intensity-%s_%s_psedo.csv\" % (param.au_name,pose,phase ) )\n",
    "\n",
    "            \n",
    "    intensity_net_best_filename = \"./model/intensity_net_%s_epoch_best_%s_trial_%d.pt\" % (param.au_name, \"posesall\", best_trial )\n",
    "\n",
    "            \n",
    "            \n",
    "    fp = open(\"./model/intensity_net_param_lst_trial_%d.json\" % best_trial)\n",
    "    intensity_net_param = json.load( fp )[param.au_name]\n",
    "    fp.close()\n",
    "            \n",
    "            \n",
    "    make_dataset_for_intensity_net( param.phase_full_lst, param )\n",
    "    dataloaders_intensity_conv = get_dataloaders_for_intensity_net( intensity_net_param, param.phase_full_lst, param )\n",
    "    predict_intensity_net( dataloaders_intensity_conv, intensity_net_best_filename, param.phase_full_lst, param )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc572ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    param = load_param()\n",
    "    \n",
    "    if not(param.data_load_path is None):\n",
    "        \n",
    "        init_and_copy_dir(param.data_load_path + \"/\" + param.dataset_lst_dirname, param.dataset_lst_dirname)\n",
    "        init_and_copy_dir(param.data_load_path + \"/\" + \"./pred_result\", \"./pred_result\")\n",
    "        init_and_copy_dir(param.data_load_path + \"/\" + \"./log\", \"./log\")\n",
    "        init_and_copy_dir(param.data_load_path + \"/\" + \"./model\", \"./model\")        \n",
    "    \n",
    "       \n",
    "    if not param.only_intensity_net_process:\n",
    "        \n",
    "        init_dir(param.dataset_lst_dirname)\n",
    "        init_dir(\"./pred_result\")\n",
    "        init_dir(\"./log\")\n",
    "        init_dir(\"./model\")\n",
    "        \n",
    "        make_dataset( param )\n",
    "    \n",
    "    best_trial = exec_training( param )\n",
    "\n",
    "    exec_prediction( best_trial, param )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4fb433",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f638178b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df121a77-f85d-4c5f-a344-1f6d1958dd17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
